{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes a model \"linear\"? \"Linear\" in what?\n",
    "\n",
    "A model is \"linear\" if it is linear in its parameters (coefficients). This means that the y variables can be modeled as some linear combination of the x variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret the coefficient for a dummy/one-hot-encoded variable?\n",
    "\n",
    "The interpretation depends on how the intercept is handled. In a typical one-hot encoding setup, one category is chosen as the reference category and is omitted from the model. The coefficient for a dummy variable represents the change in the dependent variable compared to the reference category, holding other variables constant.\n",
    "If the intercept is included, the coefficient for a dummy variable represents the difference between the mean of that category and the mean of the reference category.\n",
    "If the intercept is removed, each coefficient represents the expected outcome for that specific category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can linear regression be used for classification? Explain why, or why not.\n",
    "\n",
    "Linear regression is not ideal for classification because it predicts continuous values rather than discrete class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are signs that your linear model is over-fitting?\n",
    "\n",
    "High R-squared on training data but poor performance on test data and very low residual error in training but high error in test data are two signs of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly explain multi-collinearity using the two-stage least squares technique.\n",
    "\n",
    "Multi-collinearity occurs when two or more predictor variables in a regression model are highly correlated, making it difficult to estimate the individual effect of each variable. Two-Stage Least Squares helps address multi-collinearity. Stage 1 is where you replace the problematic predictor with an instrumental variable that is correlated with the predictor but not with the error term. Then, for Stage 2, use the fitted values from Stage 1 as the new independent variable in the final regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can you incorporate nonlinear relationships between your target variable y and your features x into your analysis?\n",
    "\n",
    "There are several methods you can use, such as polynomial regression or logarithmic transformations to model relationships that are non-linear.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the interpretation of the intercept? A slope coefficient for a variable? The coefficient for a dummy/one-hot-encoded variable?\n",
    "\n",
    "The intercept represents the expected value of y when all independent variables are zero.The slope coefficient represents the change in y for a one-unit increase in the corresponding x, holding all other variables constant. The dummy variable coefficient represents the difference in the mean response between the category represented by the dummy variable and the reference category.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
